execute_pipeline:
    tasks = [
        @tasks.utils.set_seed,
        @tasks.load_data.load_dataset,
        @tasks.load_data.subsample_dataset,
        @tasks.models.save_upstream_embeddings,
        @tasks.load_data.process_classes,
        @tasks.load_data.get_dataloaders,
        @tasks.models.fit_model,
        @tasks.models.test_model,
    ]
    execution_order = 'sequential'

tasks.utils.set_seed.seed=%SEED

tasks.models.save_upstream_embeddings:
    saving_path = "/home/eernst/Voxceleb1/avg_embeddings"

train/torch.utils.data.DataLoader:
    shuffle=True
    batch_size=%TRAIN_BATCH_SIZE
    num_workers=%TRAIN_DATALOADER_NUM_WORKERS

val/torch.utils.data.DataLoader:
    shuffle=False
    batch_size=%VAL_BATCH_SIZE
    num_workers=%VAL_DATALOADER_NUM_WORKERS

tasks.models.S3PRLUpstreamMLPDownstreamForCls:
    optimizer_params={"lr": 1e-3}

tasks.models.fit_model:
    model_cls=@tasks.models.S3PRLUpstreamMLPDownstreamForCls
    trainer_cls=@pl.Trainer
    from_checkpoint=%INITIAL_CHECKPOINT

tasks.models.test_model:
    model_cls=@tasks.models.S3PRLUpstreamMLPDownstreamForCls
    trainer_cls=@pl.Trainer

pl.Trainer:
    logger=@pl.loggers.CSVLogger()
    devices=%DEVICE
    callbacks=[@pl.callbacks.ModelCheckpoint(), @pl.callbacks.LearningRateMonitor(), @pl.callbacks.EarlyStopping()]
    accelerator='gpu'
    accumulate_grad_batches=%GRAD_ACC
    num_sanity_val_steps=1
    precision=%PRECISION
    check_val_every_n_epoch=None

pl.callbacks.ModelCheckpoint:
    dirpath=%OUTPUT_DIR
    every_n_train_steps=%CHECKPOINT_INTERVAL

pl.loggers.CSVLogger:
    save_dir=%OUTPUT_DIR
    name='pretrain_logs'

pl.callbacks.EarlyStopping:
    monitor="val_loss"
    min_delta=0.001
    patience=10
    verbose=True
    mode="min"

tasks.load_data.get_dataloaders:
    split_function=@tasks.load_data.dataset_random_split
    dataset_key_in="filtered_dataset_metadata"
    dataset_cls={'train': @train/tasks.load_data.DictDataset, 'validation': @val/tasks.load_data.DictDataset}
    dataloader_cls={'train': @train/torch.utils.data.DataLoader, 'validation': @val/torch.utils.data.DataLoader}

tasks.load_data.process_classes.dataset_key_in="filtered_dataset_metadata"

tasks.load_data.dataset_random_split:
    proportions={'train':-1,'validation':%VAL_SET_SIZE}

tasks.load_data.DictDataset.index_mapper=@tasks.load_data.compensate_lengths
tasks.load_data.compensate_lengths.chunk_length=%MAX_AUDIO_DURATION #This will sample long audios multiple times during one epoch (duration//compensate_framing times)

tasks.load_data.load_dataset.filters=[@tasks.load_data.remove_long_audios]
tasks.load_data.remove_long_audios.limit=%FILTER_AUDIO_LENGTH
