execute_pipeline:
    tasks = [
        @tasks.utils.set_seed,
        @tasks.load_data.load_dataset,
#        @tasks.models.save_upstream_embeddings,
        @tasks.load_data.calculate_prior_distribution_entropy,
        @tasks.load_data.process_classes,
        @tasks.load_data.get_dataloaders,
        @tasks.models.calculate_dataset_upstream_mean_and_std_from_wavs,
#        @tasks.models.calculate_dataset_pooled_mean_and_std,
        @tasks.models.fit_model,
        @tasks.models.test_model,
    ]
    execution_order = 'sequential'

tasks.models.calculate_dataset_upstream_mean_and_std_from_wavs.saving_path='/home/eernst/Voxceleb1/wavlm'
tasks.models.calculate_dataset_upstream_mean_and_std_from_wavs.model_cls=@models.S3PRLUpstreamMLPDownstreamForCls

tasks.utils.set_seed.seed=%SEED

train/torch.utils.data.DataLoader:
    shuffle=True
    batch_size=%TRAIN_BATCH_SIZE
    num_workers=%TRAIN_DATALOADER_NUM_WORKERS


val/torch.utils.data.DataLoader:
    shuffle=False
    batch_size=%VAL_BATCH_SIZE
    num_workers=%VAL_DATALOADER_NUM_WORKERS

test/torch.utils.data.DataLoader:
    shuffle=False
    batch_size=%TEST_BATCH_SIZE
    num_workers=%TEST_DATALOADER_NUM_WORKERS

torch.optim.Adam:
    lr=1e-4

torch.lr_scheduler.ReduceLROnPlateau:
    mode='min'
    patience=5
    factor=0.5

models.DownstreamForCls:
    hidden_layers=0

models.S3PRLUpstreamMLPDownstreamForCls:
    optimizer=@torch.optim.Adam
    downstream_cls=@models.DownstreamForCls

tasks.models.fit_model:
    model_cls=@models.S3PRLUpstreamMLPDownstreamForCls
    trainer_cls=@pl.Trainer
    from_checkpoint=%INITIAL_CHECKPOINT

tasks.models.test_model:
    model_cls=@models.S3PRLUpstreamMLPDownstreamForCls
    trainer_cls=@pl.Trainer

pl.Trainer:
    logger=@pl.loggers.CSVLogger()
    devices=%DEVICE
#    callbacks=[@pl.callbacks.ModelCheckpoint()]
    callbacks=[@pl.callbacks.ModelCheckpoint(), @pl.callbacks.EarlyStopping()]
    accelerator='gpu'
    accumulate_grad_batches=%GRAD_ACC
    num_sanity_val_steps=1
    precision=%PRECISION
    max_epochs=-1
    max_steps=%MAX_TRAIN_STEPS
    val_check_interval=%CHECKPOINT_INTERVAL
    check_val_every_n_epoch=None

pl.callbacks.ModelCheckpoint:
    dirpath=%OUTPUT_DIR
    monitor="val_loss"
    mode="min"
    save_top_k=1
#    save_top_k=-1
    save_on_train_epoch_end=False

pl.loggers.CSVLogger:
    save_dir=%OUTPUT_DIR
    name='pretrain_logs'

pl.callbacks.EarlyStopping:
    monitor="val_normalized_loss"
    min_delta=0.0001
    patience=10
    mode="min"